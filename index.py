from collections import defaultdict
import math
import re
import xml.etree.ElementTree as et
import sys
import file_io
from nltk.corpus import stopwords
STOP_WORDS = set(stopwords.words("english"))
from nltk.stem import PorterStemmer
nltk_test = PorterStemmer()

class index:
	#when running in the interactive window, initialize a test indexer with
	#the parameters "2pageWiki.xml" (or any other xml), "title.txt"
	def __init__(self, XMLfile: str, titlefile: str, docsfile: str, wordsfile: str):
		"""Initializes various global variables and calls on the load() method

		Parameters: 
		XMLFile -- the XML file that will be referred to
		titleFile --- the titles.txt file that will be written
		docsFile -- the documents.txt file that will be written
		wordsFile -- the words.txt file that will be written

		Returns:
		N/A

		Throws:
		N/A
		"""
		#Global variables
		self.XMLfile = XMLfile
		self.titlefile = titlefile
		self.docsfile = docsfile
		self.wordsfile = wordsfile
		self.root: et.Element = et.parse(XMLfile).getroot()
		self.all_pages: et.ElementTree = self.root.findall("page")
		self.pageid_to_text = {}
		self.title_list = []
		self.title_to_links = {}
		self.pagecount = 0

		#Start writing the files
		self.load()

	#This method involves parsing, tokenizing, removing stop words, and stemming
	def processing_text(self):
		"""Parses, tokenizes, removes, stop words, and stems from the XML file, thereby creating a data structures containing words, titles, id's, links

		Parameters: 
		N/A

		Returns:
		N/A

		Throws:
		N/A
		"""
		n_regex = '''\[\[[^\[]+?\]\]|[a-zA-Z0-9]+'[a-zA-Z0-9]+|[a-zA-Z0-9]+'''

		for page in self.all_pages:
			page_id = int(page.find('id').text.strip()) #id of the particular page
			list_of_processed_words = [] #List of all words in the title + text (allow duplicates)
			parsed_title = page.find('title').text.strip() #Strip the title
			parsed_text = page.find('text').text #Text of the page

			#Obtains a list of all the words, links, and non-links in this page
			words_in_titles = re.findall(n_regex, parsed_title)
			links = [n.lower() for n in re.findall(n_regex, parsed_text)]
			links = [word for word in links if "[[" in word and "]]" in word]
			non_links = [n.lower() for n in re.findall(n_regex, parsed_text)]
			non_links = [word for word in non_links if "[[" not in word or "]]" not in word]
			words = words_in_titles + non_links

			#Removing stop words and stemming
			for word in words:
				if word not in STOP_WORDS:
					stemmed_word = nltk_test.stem(word)
					list_of_processed_words.append(stemmed_word) #Adds to the word list in this page

			#Create the list of links with valid structure
			i = 0
			while i < len(links):
				if "|" in links[i]:
					links[i] = links[i].split("|")[0]+"]]"
				if("[[" in links[i] and "]]" in links[i]):
					start = links[i].index("[[")+2
					end = links[i].index("]]")
					links[i] = links[i][start:end]
				links[i] = links[i].strip()

				#If the link is the same as the title, then remove it
				if(links[i] == parsed_title.lower()):
					links.remove(links[i])
					i-=1
				#If the link does not exist the list of titles, then remove it
				elif(links[i] not in self.title_list):
					links.remove(links[i])
					i-=1
				i+=1

			#Non_duplicate_links is a list of links without duplicates
			non_duplicate_links = []
			[non_duplicate_links.append(x) for x in links if x not in non_duplicate_links] 

			#Add the new list of links to the dict
			self.title_to_links[parsed_title.lower()] = non_duplicate_links

			#Add page-word_list pair to pageid_to_text dictionary
			self.pageid_to_text[page_id] = list_of_processed_words


	def word_to_frequency(self, list_of_words):
		"""Tracks the # of appearances of every word from the XML file and calculates their frequencies

		Parameters: 
		list_of_words -- the list of unique words from the pages within the XML file

		Returns:
		word_to_frequency which, for all words of the XML file, houses # of apppearances of each unique word divided by the max # of appearances of any words from the XML file

		Throws:
		N/A
		"""
		word_to_frequency = {}
		#Tracks the number of appearances of all words
		for word in list_of_words:
			if word in word_to_frequency:
				word_to_frequency[word] += 1
			else:
				word_to_frequency[word] = 1
		max_count = max(word_to_frequency.values()) #Max number of appearances of any word
		#Calculate the frequency by dividing the apperances of the word, by the max of any word
		for key, value in word_to_frequency.items():
			new_value = round(value/max_count, 5)
			word_to_frequency[key] = new_value
		return word_to_frequency


	def calculate_term_relevances(self):
		"""Creates dictionary of relevances for words from the XML file

		Parameters: 
		N/A

		Returns:
		word_to_id_to_frequency which is generated by calculating the relevances based on word frequencies, pagecount, and # of words.

		Throws:
		N/A
		"""
		id_to_word_to_frequency = {}
		for page_id, text in self.pageid_to_text.items():
			id_to_word_to_frequency[page_id] = self.word_to_frequency(text) #Get frequencies of all words
		word_to_id_to_frequency = defaultdict(dict)
		#Calculates the relevances of words based on frequencies, pagecounts, and # of words
		for page_id, word_to_frequency in id_to_word_to_frequency.items():
			for word, frequency in word_to_frequency.items():
				word_to_id_to_frequency[word][page_id] = frequency
				i_t_f = math.log(self.pagecount/len(word_to_id_to_frequency[word]))
				word_to_id_to_frequency[word][page_id] = round(frequency * i_t_f, 5)		
		return word_to_id_to_frequency


	#A method that converts a dictionary of page to a list of pages it is
	#connected to. The values of the dictionary would not be empty.
	# Example:
	# Input: {a: [b, c], b: [a], c: [b]}
	# n_k: [2, 1, 1]
	# Output: {[[0.05, 0.05, 0.05], [0.05, 0.05, 0.05], [0.05, 0.05, 0.05]]}
	def dict_to_weight_mat(self):
		"""Creates a list of weights of pages from the XML file is connected to 

		Parameters: 
		N/A

		Returns:
		weight_mat which is the converted list from of the weights of every page 

		Throws:
		N/A
		"""
		epsilon = 0.15
		total_pages = self.pagecount
		weight_mat = [[epsilon/total_pages for i in range(total_pages)] for j in range(total_pages)]
		for (start_page, page_lst) in self.title_to_links.items():
			start_page_id = self.title_list.index(start_page)
			#Calculate the weights of words using the equation given in the handout
			if page_lst == []: 
				weight_mat[start_page_id] = [round(epsilon/total_pages + (1-epsilon)/(total_pages - 1), 5)] * total_pages
				weight_mat[start_page_id][start_page_id] = round(epsilon/total_pages, 5)
			else:
				num_ends = len(page_lst)
				for end_page in page_lst:
					lowered_title_list = [n.lower() for n in self.title_list]
					end_page_id = lowered_title_list.index(end_page)
					weight_mat[start_page_id][end_page_id] = round(epsilon/total_pages + (1-epsilon)/num_ends, 5)
		return weight_mat

	def pagerank(self, weight_mat):
		"""Conducts pagerank to rank every page from the XML file

		Parameters: 
		weight_mat -- the list of weights of every link to one another that was created in dict_to_weights_mat

		Returns:
		r_ a list which represents the rank the every page from the XML file

		Throws:
		N/A
		"""
		#Page rank algorithm
		delta = 0.001
		r = [0] * self.pagecount #Stores the rankings from the previous iteration
		r_ = [1/self.pagecount] * self.pagecount #Stores the current rankings
		while (math.dist(r, r_) > delta):
			r = r_.copy() #Update r to be r_
			for j in range(self.pagecount):
				r_[j] = 0
				for k in range(self.pagecount):
					#Consider the summation equation as given in the handout
					r_[j] = r_[j] + weight_mat[k][j] * r[k]
		return r_
	
	def load(self):
		"""Helps write the titles.txt, documents.txt, and words.txt files by calling on the various methods within index.py, thereby filling in various data structures that served as parameters or components in other methods

		Parameters: 
		N/A

		Returns:
		N/A

		Throws:
		N/A
		"""

		pageid_to_title = {}
		
		#List of all ids 
		ids_list = []
		for page in self.all_pages:			
			parsed_title = page.find('title').text.strip()
			id = int(page.find('id').text)
			pageid_to_title[id] = parsed_title
			ids_list.append(id)
			self.title_list.append(parsed_title.lower())
		#Record the page count, which is just the number of titles in the list
		self.pagecount = len(self.title_list)

		self.processing_text() #Processes text for links, words, titles, etc.

		#Get the page rank with the created weight data structure and pagerank method
		pagerank = self.pagerank(self.dict_to_weight_mat())
		pageid_to_pagerank = {ids_list[i]: pagerank[i] for i in range(self.pagecount)}
			
		file_io.write_title_file(self.titlefile, pageid_to_title) #Write title file

		file_io.write_words_file(self.wordsfile, self.calculate_term_relevances()) #Write words file

		file_io.write_docs_file(self.docsfile, pageid_to_pagerank) #Write documents file

#Main Method
if __name__ == "__main__":
	"""Main method that creates the index object depending on the user's input

    Parameters: 
    N/A

	Returns:
    N/A

	Throws:
	N/A
	"""
	#If the input is of length 5, instantiate index object
	if(len(sys.argv) == 5):
		i = index(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])
	#Otherwise, output error message
	else:
		print("Retry! Bad input.")